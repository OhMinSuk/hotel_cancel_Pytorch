# -*- coding: utf-8 -*-
"""hotel_cancellation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aVh8j2y-aclA1e1tSENV0mHq2sKUXUOl

# 머신러닝 프로젝트 202447001 오민석
# 호텔 예약 취소 예측 모델
"""

import kagglehub
import pandas as pd
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
!apt-get -qq -y install fonts-nanum > /dev/null

fe = fm.FontEntry(
    fname=r'/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf',
    name='NanumGothic')
fm.fontManager.ttflist.insert(0, fe)
plt.rcParams.update({'font.size': 12, 'font.family': 'NanumGothic'})

df = pd.read_csv("/content/hotel_booking.csv")

df.shape

df.info()

df.describe()

"""## 데이터 전처리

개인정보 및 불필요한 컬럼 삭제
"""

df.drop(['name','email','phone-number','credit_card','reservation_status_date','reservation_status'],axis=1,inplace=True)

df.tail()

print(df.isnull().sum()[df.isnull().sum() > 0])

"""결측치 대체"""

df['children'] = df['children'].fillna(0)

df['company'] = df['company'].fillna(0) # 0은 'no company'를 의미

df['agent'] = df['agent'].fillna(0)  # 0은 'no agent'를 의미

df['country'] = df['country'].fillna(df['country'].mode()[0])

df.shape

"""## EDA"""

# 'is_canceled'가 1인 값만 필터링
cancel_count = df['is_canceled'].value_counts()
cancelled = cancel_count[1]  # 취소된 예약의 수 (is_canceled == 1)

# 파이 차트 생성
plt.figure(figsize=(5, 5))
plt.pie([cancelled, cancel_count[0]],  # 취소된 예약과 취소되지 않은 예약
        labels=['취소', '취소 안됨'],
        autopct='%1.1f%%',
        colors=sns.color_palette('Pastel1'))
plt.title('호텔 예약 취소 여부 비율', pad=20, size=14)
plt.show()

print(f"취소되지 않은 예약: {cancel_count.get(0, 0)}개")
print(f"취소된 예약: {cancel_count.get(1, 0)}개")

"""가독성을 위해 market_segment 데이터 한국어로 맵핑"""

market_segment_mapping = {
    'Online TA': '온라인 여행사',
    'Offline TA/TO': '오프라인 여행사',
    'Groups': '단체 예약',
    'Direct': '직접 예약',
    'Corporate': '기업 예약',
    'Complementary': '무료 제공',
    'Aviation': '항공사 예약',
    'Undefined': '미정의'
}

# market_segment를 매핑하여 카운트를 계산합니다.
ms_count = df['market_segment'].map(market_segment_mapping).value_counts()

# 색상 팔레트 지정
colors = sns.color_palette('muted')

# 바 차트 생성
plt.figure(figsize=(8, 7))
bars = plt.bar(ms_count.index, ms_count.values, color=colors)

# 값 레이블 추가
for bar in bars:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2., height,
            f'{int(height):,}건',
            ha='center', va='bottom')

plt.xticks(rotation=45, ha='right')
plt.title('예약방법 분포', pad=20, size=14)
plt.ylabel('예약 수', size=12)

# y축 천 단위 구분기호 추가
plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: format(int(x), ',')))

plt.tight_layout()
plt.show()

df['market_segment_kr'] = df['market_segment'].map(market_segment_mapping)
cancellation_by_segment = pd.crosstab(df['market_segment_kr'], df['is_canceled'])
cancellation_count = cancellation_by_segment[1].sort_values(ascending=False)

# 바 차트 생성
plt.figure(figsize=(8, 7))
bars = plt.bar(cancellation_count.index, cancellation_count.values, color=colors)

# 값 레이블 추가 (간격 줄임)
for bar in bars:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2., height + 50,  # 간격을 50으로 조정
            f'{int(height):,}건',
            ha='center', va='bottom')

plt.xticks(rotation=45, ha='right')
plt.title('예약방법별 취소 건수', pad=20, size=14)
plt.ylabel('취소 건수', size=12)

# y축 천 단위 구분기호 추가
plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: format(int(x), ',')))

plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 6))
sns.boxplot(x='is_canceled', y='lead_time', data=df)
plt.title('예약 취소 여부에 따른 예약에서 체크인까지의 기간 분포')
plt.show()

# 'arrival_date_month'와 'hotel'을 기준으로 월별 예약 수를 구분하여 집계
monthly_bookings_by_hotel = df.groupby(['arrival_date_month', 'hotel']).size().unstack(fill_value=0)

# 월 순서를 맞추기 위해 'arrival_date_month'를 1월부터 12월까지 정렬
monthly_bookings_by_hotel = monthly_bookings_by_hotel.reindex([
    'December', 'November', 'October', 'September', 'August', 'July',
    'June', 'May', 'April', 'March', 'February', 'January'
])

# 시각화
monthly_bookings_by_hotel.plot(kind='barh', stacked=True, figsize=(15, 6), color=sns.color_palette("tab10"))
plt.title('호텔 종류별 월별 예약 수')
plt.xlabel('예약 수')
plt.ylabel('월')
plt.xticks(rotation=45)
plt.legend(title='Hotel', loc='upper right')
plt.tight_layout()
plt.show()

# 'is_canceled'이 1인 데이터 필터링
canceled_bookings = df[df['is_canceled'] == 1]

# 'arrival_date_month' 기준으로 월별 취소된 예약 수 집계
monthly_canceled_bookings = canceled_bookings['arrival_date_month'].value_counts()

# 월 순서를 맞추기 위해 재정렬
monthly_canceled_bookings = monthly_canceled_bookings.reindex([
    'December', 'November', 'October', 'September', 'August', 'July',
    'June', 'May', 'April', 'March', 'February', 'January'
])

# 시각화
monthly_canceled_bookings.plot(kind='barh', figsize=(12, 6), color=sns.color_palette("Paired"))
plt.title('월별 취소된 예약 수')
plt.xlabel('취소된 예약 수')
plt.ylabel('월')
plt.tight_layout()
plt.show()

"""# 예측 모델 생성"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from sklearn.preprocessing import StandardScaler, LabelEncoder
from copy import deepcopy

# 범주형 변수 전처리
def preprocess_data(df):

    categorical_columns = ['hotel', 'arrival_date_month', 'meal', 'country',
                         'market_segment', 'distribution_channel', 'reserved_room_type',
                         'assigned_room_type', 'deposit_type', 'customer_type']

    # Label Encoding
    for column in categorical_columns:
        le = LabelEncoder()
        df[column] = le.fit_transform(df[column])

    # target 변수 분리
    y = df['is_canceled'].values
    X = df.drop('is_canceled', axis=1).values

    # numpy array를 torch tensor로 변환
    data = torch.from_numpy(np.column_stack((X, y))).float()

    return data

# 데이터 준비
data = preprocess_data(df)
x = data[:, :-1]
y = data[:, -1:]

# 데이터 분할 비율 설정
ratios = [0.6, 0.2, 0.2]
train_cnt = int(data.size(0) * ratios[0])
valid_cnt = int(data.size(0) * ratios[1])
test_cnt = data.size(0) - train_cnt - valid_cnt
cnts = [train_cnt, valid_cnt, test_cnt]

print("Train %d / Valid %d / Test %d " % (train_cnt, valid_cnt, test_cnt))

# 데이터 섞기
indices = torch.randperm(data.size(0))
x = torch.index_select(x, dim=0, index=indices)
y = torch.index_select(y, dim=0, index=indices)

# 데이터셋 분할
x = x.split(cnts, dim=0)
y = y.split(cnts, dim=0)

# 데이터 스케일링
scaler = StandardScaler()
scaler.fit(x[0].numpy())
x = [
    torch.from_numpy(scaler.transform(x[0].numpy())).float(),
    torch.from_numpy(scaler.transform(x[1].numpy())).float(),
    torch.from_numpy(scaler.transform(x[2].numpy())).float()
]

# 입출력 차원 확인
input_size = x[0].size(-1)
output_size = y[0].size(-1)
print(f"Input size: {input_size}")
print(f"Output size: {output_size}")

"""## 기본 모델

batch size 128
"""

# 모델 정의
model = nn.Sequential(
    nn.Linear(input_size, 64),
    nn.LeakyReLU(),
    nn.Linear(64, 32),
    nn.LeakyReLU(),
    nn.Linear(32, 16),
    nn.LeakyReLU(),
    nn.Linear(16, 8),
    nn.LeakyReLU(),
    nn.Linear(8, 1),
    nn.Sigmoid(),
)

# 옵티마이저 설정
optimizer = optim.Adam(model.parameters())

# 학습 파라미터 설정
n_epochs = 1000
batch_size = 128
print_interval = 10
early_stop = 100

# 학습 기록을 위한 변수들
lowest_loss = np.inf
best_model = None
lowest_epoch = np.inf
train_history, valid_history = [], []

# 학습 루프
for i in range(n_epochs):
    # 학습 데이터 섞기
    indices = torch.randperm(x[0].size(0))
    x_ = torch.index_select(x[0], dim=0, index=indices)
    y_ = torch.index_select(y[0], dim=0, index=indices)

    # 배치 생성
    x_ = x_.split(batch_size, dim=0)
    y_ = y_.split(batch_size, dim=0)

    # 학습
    train_loss = 0
    for x_i, y_i in zip(x_, y_):
        y_hat_i = model(x_i)
        loss = F.binary_cross_entropy(y_hat_i, y_i)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        train_loss += float(loss)

    train_loss = train_loss / len(x_)

    # 검증
    with torch.no_grad():
        x_ = x[1].split(batch_size, dim=0)
        y_ = y[1].split(batch_size, dim=0)

        valid_loss = 0
        y_hat = []

        for x_i, y_i in zip(x_, y_):
            y_hat_i = model(x_i)
            loss = F.binary_cross_entropy(y_hat_i, y_i)

            valid_loss += float(loss)
            y_hat += [y_hat_i]

        valid_loss = valid_loss / len(x_)

    train_history += [train_loss]
    valid_history += [valid_loss]

    if (i + 1) % print_interval == 0:
        print('Epoch %d: train loss=%.4e valid_loss=%.4e lowest_loss=%.4e' % (
            i + 1,
            train_loss,
            valid_loss,
            lowest_loss,
        ))

    # 모델 저장 및 Early Stopping
    if valid_loss <= lowest_loss:
        lowest_loss = valid_loss
        lowest_epoch = i
        best_model = deepcopy(model.state_dict())
    else:
        if early_stop > 0 and lowest_epoch + early_stop < i + 1:
            print("There is no improvement during last %d epochs." % early_stop)
            break

print("The best validation loss from epoch %d: %.4e" % (lowest_epoch + 1, lowest_loss))
model.load_state_dict(best_model)

# 테스트 데이터로 평가 및 정확도 계산
with torch.no_grad():
    x_ = x[2].split(batch_size, dim=0)
    y_ = y[2].split(batch_size, dim=0)

    test_loss = 0
    correct = 0
    total = 0

    for x_i, y_i in zip(x_, y_):
        y_hat_i = model(x_i)
        loss = F.binary_cross_entropy(y_hat_i, y_i)

        # 정확도 계산
        predicted = (y_hat_i > 0.5).float()
        total += y_i.size(0)
        correct += (predicted == y_i).sum().item()

        test_loss += float(loss)

    test_loss = test_loss / len(x_)
    accuracy = 100 * correct / total

print(f"Test loss: {test_loss:.4e}")
print(f"Test accuracy: {accuracy:.2f}%")

plt.figure(figsize=(10, 6))
plt.plot(train_history, label='Train Loss')
plt.plot(valid_history, label='Validation Loss')
plt.title('Training and Validation Loss Over Time')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

"""## DropOut 적용 모델

batch size 128
"""

# 모델 정의
p = 0.03
model = nn.Sequential(
    nn.Linear(input_size,64),
    nn.LeakyReLU(),
    nn.Dropout(p),
    nn.Linear(64, 32),
    nn.LeakyReLU(),
    nn.Dropout(p),
    nn.Linear(32, 16),
    nn.LeakyReLU(),
    nn.Dropout(p),
    nn.Linear(16, 8),
    nn.LeakyReLU(),
    nn.Dropout(p),
    nn.Linear(8, 1),
    nn.Sigmoid(),
)

# 옵티마이저 설정
optimizer = optim.Adam(model.parameters())

# 학습 파라미터 설정
n_epochs = 1000
batch_size = 128
print_interval = 10
early_stop = 100

# 학습 기록을 위한 변수들
lowest_loss = np.inf
best_model = None
lowest_epoch = np.inf
train_history, valid_history = [], []

# 학습 루프
for i in range(n_epochs):
    # 학습 데이터 섞기
    indices = torch.randperm(x[0].size(0))
    x_ = torch.index_select(x[0], dim=0, index=indices)
    y_ = torch.index_select(y[0], dim=0, index=indices)

    # 배치 생성
    x_ = x_.split(batch_size, dim=0)
    y_ = y_.split(batch_size, dim=0)

    # 학습
    train_loss = 0
    for x_i, y_i in zip(x_, y_):
        y_hat_i = model(x_i)
        loss = F.binary_cross_entropy(y_hat_i, y_i)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        train_loss += float(loss)

    train_loss = train_loss / len(x_)

    # 검증
    with torch.no_grad():
        x_ = x[1].split(batch_size, dim=0)
        y_ = y[1].split(batch_size, dim=0)

        valid_loss = 0
        y_hat = []

        for x_i, y_i in zip(x_, y_):
            y_hat_i = model(x_i)
            loss = F.binary_cross_entropy(y_hat_i, y_i)

            valid_loss += float(loss)
            y_hat += [y_hat_i]

        valid_loss = valid_loss / len(x_)

    train_history += [train_loss]
    valid_history += [valid_loss]

    if (i + 1) % print_interval == 0:
        print('Epoch %d: train loss=%.4e valid_loss=%.4e lowest_loss=%.4e' % (
            i + 1,
            train_loss,
            valid_loss,
            lowest_loss,
        ))

    # 모델 저장 및 Early Stopping
    if valid_loss <= lowest_loss:
        lowest_loss = valid_loss
        lowest_epoch = i
        best_model = deepcopy(model.state_dict())
    else:
        if early_stop > 0 and lowest_epoch + early_stop < i + 1:
            print("There is no improvement during last %d epochs." % early_stop)
            break

print("The best validation loss from epoch %d: %.4e" % (lowest_epoch + 1, lowest_loss))
model.load_state_dict(best_model)

# 테스트 데이터로 평가 및 정확도 계산
with torch.no_grad():
    x_ = x[2].split(batch_size, dim=0)
    y_ = y[2].split(batch_size, dim=0)

    test_loss = 0
    correct = 0
    total = 0

    for x_i, y_i in zip(x_, y_):
        y_hat_i = model(x_i)
        loss = F.binary_cross_entropy(y_hat_i, y_i)

        # 정확도 계산
        predicted = (y_hat_i > 0.5).float()
        total += y_i.size(0)
        correct += (predicted == y_i).sum().item()

        test_loss += float(loss)

    test_loss = test_loss / len(x_)
    accuracy = 100 * correct / total

print(f"Test loss: {test_loss:.4e}")
print(f"Test accuracy: {accuracy:.2f}%")

plt.figure(figsize=(10, 6))
plt.plot(train_history, label='Train Loss')
plt.plot(valid_history, label='Validation Loss')
plt.title('Training and Validation Loss Over Time')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

"""## Batch Normalization 적용 모델

batch size 128
"""

# BatchNormalization만 적용한 모델
model = nn.Sequential(
    nn.Linear(input_size, 64),
    nn.LeakyReLU(),
    nn.BatchNorm1d(64),

    nn.Linear(64, 32),
    nn.LeakyReLU(),
    nn.BatchNorm1d(32),

    nn.Linear(32, 16),
    nn.LeakyReLU(),
    nn.BatchNorm1d(16),

    nn.Linear(16, 8),
    nn.LeakyReLU(),
    nn.BatchNorm1d(8),

    nn.Linear(8, 1),
    nn.Sigmoid(),
)

# 옵티마이저 설정
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 학습 파라미터
n_epochs = 1000
batch_size = 128
print_interval = 10
early_stop = 100

# 학습 기록을 위한 변수들
lowest_loss = np.inf
best_model = None
lowest_epoch = np.inf
train_history, valid_history = [], []

# 학습 루프
for i in range(n_epochs):
    indices = torch.randperm(x[0].size(0))
    x_ = torch.index_select(x[0], dim=0, index=indices)
    y_ = torch.index_select(y[0], dim=0, index=indices)

    x_ = x_.split(batch_size, dim=0)
    y_ = y_.split(batch_size, dim=0)

    # 학습 모드
    model.train()
    train_loss = 0
    for x_i, y_i in zip(x_, y_):
        y_hat_i = model(x_i)
        loss = F.binary_cross_entropy(y_hat_i, y_i)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        train_loss += float(loss)

    train_loss = train_loss / len(x_)

    # 평가 모드
    model.eval()
    with torch.no_grad():
        x_ = x[1].split(batch_size, dim=0)
        y_ = y[1].split(batch_size, dim=0)

        valid_loss = 0

        for x_i, y_i in zip(x_, y_):
            y_hat_i = model(x_i)
            loss = F.binary_cross_entropy(y_hat_i, y_i)
            valid_loss += float(loss)

        valid_loss = valid_loss / len(x_)

    train_history += [train_loss]
    valid_history += [valid_loss]

    if (i + 1) % print_interval == 0:
        print('Epoch %d: train loss=%.4e valid_loss=%.4e lowest_loss=%.4e' % (
            i + 1,
            train_loss,
            valid_loss,
            lowest_loss,
        ))

    if valid_loss <= lowest_loss:
        lowest_loss = valid_loss
        lowest_epoch = i
        best_model = deepcopy(model.state_dict())
    else:
        if early_stop > 0 and lowest_epoch + early_stop < i + 1:
            print("There is no improvement during last %d epochs." % early_stop)
            break

print("The best validation loss from epoch %d: %.4e" % (lowest_epoch + 1, lowest_loss))
model.load_state_dict(best_model)

# 테스트 데이터로 평가
model.eval()
with torch.no_grad():
    x_ = x[2].split(batch_size, dim=0)
    y_ = y[2].split(batch_size, dim=0)

    test_loss = 0
    correct = 0
    total = 0

    for x_i, y_i in zip(x_, y_):
        y_hat_i = model(x_i)
        loss = F.binary_cross_entropy(y_hat_i, y_i)

        predicted = (y_hat_i > 0.5).float()
        total += y_i.size(0)
        correct += (predicted == y_i).sum().item()

        test_loss += float(loss)

    test_loss = test_loss / len(x_)
    accuracy = 100 * correct / total

print(f"Test loss: {test_loss:.4e}")
print(f"Test accuracy: {accuracy:.2f}%")

# 손실 그래프 그리기
plt.figure(figsize=(10, 6))
plt.plot(train_history, label='Train Loss')
plt.plot(valid_history, label='Validation Loss')
plt.title('Training and Validation Loss Over Time')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

"""## batch size 64 (최고 성능)"""

# BatchNormalization만 적용한 모델
model = nn.Sequential(
    nn.Linear(input_size, 64),
    nn.LeakyReLU(),
    nn.BatchNorm1d(64),

    nn.Linear(64, 32),
    nn.LeakyReLU(),
    nn.BatchNorm1d(32),

    nn.Linear(32, 16),
    nn.LeakyReLU(),
    nn.BatchNorm1d(16),

    nn.Linear(16, 8),
    nn.LeakyReLU(),
    nn.BatchNorm1d(8),

    nn.Linear(8, 1),
    nn.Sigmoid(),
)

# 옵티마이저 설정
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 학습 파라미터
n_epochs = 1000
batch_size = 64
print_interval = 10
early_stop = 100

# 학습 기록을 위한 변수들
lowest_loss = np.inf
best_model = None
lowest_epoch = np.inf
train_history, valid_history = [], []

# 학습 루프
for i in range(n_epochs):
    indices = torch.randperm(x[0].size(0))
    x_ = torch.index_select(x[0], dim=0, index=indices)
    y_ = torch.index_select(y[0], dim=0, index=indices)

    x_ = x_.split(batch_size, dim=0)
    y_ = y_.split(batch_size, dim=0)

    # 학습 모드
    model.train()
    train_loss = 0
    for x_i, y_i in zip(x_, y_):
        y_hat_i = model(x_i)
        loss = F.binary_cross_entropy(y_hat_i, y_i)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        train_loss += float(loss)

    train_loss = train_loss / len(x_)

    # 평가 모드
    model.eval()
    with torch.no_grad():
        x_ = x[1].split(batch_size, dim=0)
        y_ = y[1].split(batch_size, dim=0)

        valid_loss = 0

        for x_i, y_i in zip(x_, y_):
            y_hat_i = model(x_i)
            loss = F.binary_cross_entropy(y_hat_i, y_i)
            valid_loss += float(loss)

        valid_loss = valid_loss / len(x_)

    train_history += [train_loss]
    valid_history += [valid_loss]

    if (i + 1) % print_interval == 0:
        print('Epoch %d: train loss=%.4e valid_loss=%.4e lowest_loss=%.4e' % (
            i + 1,
            train_loss,
            valid_loss,
            lowest_loss,
        ))

    if valid_loss <= lowest_loss:
        lowest_loss = valid_loss
        lowest_epoch = i
        best_model = deepcopy(model.state_dict())
    else:
        if early_stop > 0 and lowest_epoch + early_stop < i + 1:
            print("There is no improvement during last %d epochs." % early_stop)
            break

print("The best validation loss from epoch %d: %.4e" % (lowest_epoch + 1, lowest_loss))
model.load_state_dict(best_model)

# 테스트 데이터로 평가
model.eval()
with torch.no_grad():
    x_ = x[2].split(batch_size, dim=0)
    y_ = y[2].split(batch_size, dim=0)

    test_loss = 0
    correct = 0
    total = 0

    for x_i, y_i in zip(x_, y_):
        y_hat_i = model(x_i)
        loss = F.binary_cross_entropy(y_hat_i, y_i)

        predicted = (y_hat_i > 0.5).float()
        total += y_i.size(0)
        correct += (predicted == y_i).sum().item()

        test_loss += float(loss)

    test_loss = test_loss / len(x_)
    accuracy = 100 * correct / total

print(f"Test loss: {test_loss:.4e}")
print(f"Test accuracy: {accuracy:.2f}%")

# 손실 그래프 그리기
plt.figure(figsize=(10, 6))
plt.plot(train_history, label='Train Loss')
plt.plot(valid_history, label='Validation Loss')
plt.title('Training and Validation Loss Over Time')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

"""batch size 256"""

# BatchNormalization만 적용한 모델
model = nn.Sequential(
    nn.Linear(input_size, 64),
    nn.LeakyReLU(),
    nn.BatchNorm1d(64),

    nn.Linear(64, 32),
    nn.LeakyReLU(),
    nn.BatchNorm1d(32),

    nn.Linear(32, 16),
    nn.LeakyReLU(),
    nn.BatchNorm1d(16),

    nn.Linear(16, 8),
    nn.LeakyReLU(),
    nn.BatchNorm1d(8),

    nn.Linear(8, 1),
    nn.Sigmoid(),
)

# 옵티마이저 설정
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 학습 파라미터
n_epochs = 1000
batch_size = 256
print_interval = 10
early_stop = 100

# 학습 기록을 위한 변수들
lowest_loss = np.inf
best_model = None
lowest_epoch = np.inf
train_history, valid_history = [], []

# 학습 루프
for i in range(n_epochs):
    indices = torch.randperm(x[0].size(0))
    x_ = torch.index_select(x[0], dim=0, index=indices)
    y_ = torch.index_select(y[0], dim=0, index=indices)

    x_ = x_.split(batch_size, dim=0)
    y_ = y_.split(batch_size, dim=0)

    # 학습 모드
    model.train()
    train_loss = 0
    for x_i, y_i in zip(x_, y_):
        y_hat_i = model(x_i)
        loss = F.binary_cross_entropy(y_hat_i, y_i)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        train_loss += float(loss)

    train_loss = train_loss / len(x_)

    # 평가 모드
    model.eval()
    with torch.no_grad():
        x_ = x[1].split(batch_size, dim=0)
        y_ = y[1].split(batch_size, dim=0)

        valid_loss = 0

        for x_i, y_i in zip(x_, y_):
            y_hat_i = model(x_i)
            loss = F.binary_cross_entropy(y_hat_i, y_i)
            valid_loss += float(loss)

        valid_loss = valid_loss / len(x_)

    train_history += [train_loss]
    valid_history += [valid_loss]

    if (i + 1) % print_interval == 0:
        print('Epoch %d: train loss=%.4e valid_loss=%.4e lowest_loss=%.4e' % (
            i + 1,
            train_loss,
            valid_loss,
            lowest_loss,
        ))

    if valid_loss <= lowest_loss:
        lowest_loss = valid_loss
        lowest_epoch = i
        best_model = deepcopy(model.state_dict())
    else:
        if early_stop > 0 and lowest_epoch + early_stop < i + 1:
            print("There is no improvement during last %d epochs." % early_stop)
            break

print("The best validation loss from epoch %d: %.4e" % (lowest_epoch + 1, lowest_loss))
model.load_state_dict(best_model)

# 테스트 데이터로 평가
model.eval()
with torch.no_grad():
    x_ = x[2].split(batch_size, dim=0)
    y_ = y[2].split(batch_size, dim=0)

    test_loss = 0
    correct = 0
    total = 0

    for x_i, y_i in zip(x_, y_):
        y_hat_i = model(x_i)
        loss = F.binary_cross_entropy(y_hat_i, y_i)

        predicted = (y_hat_i > 0.5).float()
        total += y_i.size(0)
        correct += (predicted == y_i).sum().item()

        test_loss += float(loss)

    test_loss = test_loss / len(x_)
    accuracy = 100 * correct / total

print(f"Test loss: {test_loss:.4e}")
print(f"Test accuracy: {accuracy:.2f}%")

# 손실 그래프 그리기
plt.figure(figsize=(10, 6))
plt.plot(train_history, label='Train Loss')
plt.plot(valid_history, label='Validation Loss')
plt.title('Training and Validation Loss Over Time')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

"""## Batch Normalization, DropOut 둘다 적용

---


"""

# Batch Normalization, DropOut 둘다 적용

model = nn.Sequential(
    nn.Linear(input_size, 64),
    nn.BatchNorm1d(64),
    nn.LeakyReLU(),
    nn.Dropout(0.3),

    nn.Linear(64, 32),
    nn.BatchNorm1d(32),
    nn.LeakyReLU(),
    nn.Dropout(0.3),

    nn.Linear(32, 16),
    nn.BatchNorm1d(16),
    nn.LeakyReLU(),
    nn.Dropout(0.3),

    nn.Linear(16, 8),
    nn.BatchNorm1d(8),
    nn.LeakyReLU(),
    nn.Dropout(0.3),

    nn.Linear(8, 1),
    nn.Sigmoid(),
)

# 옵티마이저 설정
optimizer = optim.Adam(model.parameters())

# 학습 파라미터 설정
n_epochs = 1000
batch_size = 128
print_interval = 10
early_stop = 100

# 학습 기록을 위한 변수들
lowest_loss = np.inf
best_model = None
lowest_epoch = np.inf
train_history, valid_history = [], []

# 학습 루프
for i in range(n_epochs):
    # 학습 데이터 섞기
    indices = torch.randperm(x[0].size(0))
    x_ = torch.index_select(x[0], dim=0, index=indices)
    y_ = torch.index_select(y[0], dim=0, index=indices)

    # 배치 생성
    x_ = x_.split(batch_size, dim=0)
    y_ = y_.split(batch_size, dim=0)

    # 학습
    train_loss = 0
    for x_i, y_i in zip(x_, y_):
        y_hat_i = model(x_i)
        loss = F.binary_cross_entropy(y_hat_i, y_i)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        train_loss += float(loss)

    train_loss = train_loss / len(x_)

    # 검증
    with torch.no_grad():
        x_ = x[1].split(batch_size, dim=0)
        y_ = y[1].split(batch_size, dim=0)

        valid_loss = 0
        y_hat = []

        for x_i, y_i in zip(x_, y_):
            y_hat_i = model(x_i)
            loss = F.binary_cross_entropy(y_hat_i, y_i)

            valid_loss += float(loss)
            y_hat += [y_hat_i]

        valid_loss = valid_loss / len(x_)

    train_history += [train_loss]
    valid_history += [valid_loss]

    if (i + 1) % print_interval == 0:
        print('Epoch %d: train loss=%.4e valid_loss=%.4e lowest_loss=%.4e' % (
            i + 1,
            train_loss,
            valid_loss,
            lowest_loss,
        ))

    # 모델 저장 및 Early Stopping
    if valid_loss <= lowest_loss:
        lowest_loss = valid_loss
        lowest_epoch = i
        best_model = deepcopy(model.state_dict())
    else:
        if early_stop > 0 and lowest_epoch + early_stop < i + 1:
            print("There is no improvement during last %d epochs." % early_stop)
            break

print("The best validation loss from epoch %d: %.4e" % (lowest_epoch + 1, lowest_loss))
model.load_state_dict(best_model)

# 테스트 데이터로 평가 및 정확도 계산
with torch.no_grad():
    x_ = x[2].split(batch_size, dim=0)
    y_ = y[2].split(batch_size, dim=0)

    test_loss = 0
    correct = 0
    total = 0

    for x_i, y_i in zip(x_, y_):
        y_hat_i = model(x_i)
        loss = F.binary_cross_entropy(y_hat_i, y_i)

        # 정확도 계산
        predicted = (y_hat_i > 0.5).float()
        total += y_i.size(0)
        correct += (predicted == y_i).sum().item()

        test_loss += float(loss)

    test_loss = test_loss / len(x_)
    accuracy = 100 * correct / total

print(f"Test loss: {test_loss:.4e}")
print(f"Test accuracy: {accuracy:.2f}%")

plt.figure(figsize=(10, 6))
plt.plot(train_history, label='Train Loss')
plt.plot(valid_history, label='Validation Loss')
plt.title('Training and Validation Loss Over Time')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

"""# 모델 성능 평가"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.metrics import roc_curve, precision_recall_curve, auc

# 성능이 가장 좋았던 모델

model = nn.Sequential(
    nn.Linear(input_size, 64),
    nn.LeakyReLU(),
    nn.BatchNorm1d(64),

    nn.Linear(64, 32),
    nn.LeakyReLU(),
    nn.BatchNorm1d(32),

    nn.Linear(32, 16),
    nn.LeakyReLU(),
    nn.BatchNorm1d(16),

    nn.Linear(16, 8),
    nn.LeakyReLU(),
    nn.BatchNorm1d(8),

    nn.Linear(8, 1),
    nn.Sigmoid(),
)

# 옵티마이저 설정
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 학습 파라미터
n_epochs = 1000
batch_size = 64
print_interval = 10
early_stop = 100

# 학습 기록을 위한 변수들
lowest_loss = np.inf
best_model = None
lowest_epoch = np.inf
train_history, valid_history = [], []

# 학습 루프
for i in range(n_epochs):
    indices = torch.randperm(x[0].size(0))
    x_ = torch.index_select(x[0], dim=0, index=indices)
    y_ = torch.index_select(y[0], dim=0, index=indices)

    x_ = x_.split(batch_size, dim=0)
    y_ = y_.split(batch_size, dim=0)

    # 학습 모드
    model.train()
    train_loss = 0
    for x_i, y_i in zip(x_, y_):
        y_hat_i = model(x_i)
        loss = F.binary_cross_entropy(y_hat_i, y_i)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        train_loss += float(loss)

    train_loss = train_loss / len(x_)

    # 평가 모드
    model.eval()
    with torch.no_grad():
        x_ = x[1].split(batch_size, dim=0)
        y_ = y[1].split(batch_size, dim=0)

        valid_loss = 0

        for x_i, y_i in zip(x_, y_):
            y_hat_i = model(x_i)
            loss = F.binary_cross_entropy(y_hat_i, y_i)
            valid_loss += float(loss)

        valid_loss = valid_loss / len(x_)

    train_history += [train_loss]
    valid_history += [valid_loss]

    if (i + 1) % print_interval == 0:
        print('Epoch %d: train loss=%.4e valid_loss=%.4e lowest_loss=%.4e' % (
            i + 1,
            train_loss,
            valid_loss,
            lowest_loss,
        ))

    if valid_loss <= lowest_loss:
        lowest_loss = valid_loss
        lowest_epoch = i
        best_model = deepcopy(model.state_dict())
    else:
        if early_stop > 0 and lowest_epoch + early_stop < i + 1:
            print("There is no improvement during last %d epochs." % early_stop)
            break

print("The best validation loss from epoch %d: %.4e" % (lowest_epoch + 1, lowest_loss))
model.load_state_dict(best_model)

# 테스트 데이터로 평가
model.eval()
with torch.no_grad():
    x_ = x[2].split(batch_size, dim=0)
    y_ = y[2].split(batch_size, dim=0)

    test_loss = 0
    correct = 0
    total = 0

    for x_i, y_i in zip(x_, y_):
        y_hat_i = model(x_i)
        loss = F.binary_cross_entropy(y_hat_i, y_i)

        predicted = (y_hat_i > 0.5).float()
        total += y_i.size(0)
        correct += (predicted == y_i).sum().item()

        test_loss += float(loss)

    test_loss = test_loss / len(x_)
    accuracy = 100 * correct / total

print(f"Test loss: {test_loss:.4e}")
print(f"Test accuracy: {accuracy:.2f}%")


def detailed_analysis(model, x_test, y_test, batch_size=256):
    model.eval()
    all_preds = []
    all_probs = []
    all_true = []

    # 예측값 생성
    with torch.no_grad():
        for i in range(0, len(x_test), batch_size):
            batch_x = x_test[i:i+batch_size]
            batch_y = y_test[i:i+batch_size]

            outputs = model(batch_x)
            probs = outputs.numpy()
            preds = (outputs > 0.5).float().numpy()

            all_probs.extend(probs)
            all_preds.extend(preds)
            all_true.extend(batch_y.numpy())

    all_probs = np.array(all_probs)
    all_preds = np.array(all_preds)
    all_true = np.array(all_true)

    # 1. 혼동 행렬
    cm = confusion_matrix(all_true, all_preds)

    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title('Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.show()

    # 2. 분류 보고서
    print("\nClassification Report:")
    print(classification_report(all_true, all_preds, target_names=['Not Canceled', 'Canceled']))

    # 3. ROC 곡선
    fpr, tpr, _ = roc_curve(all_true, all_probs)
    roc_auc = auc(fpr, tpr)

    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curve')
    plt.legend(loc="lower right")
    plt.show()

    # 4. Precision-Recall 곡선
    precision, recall, _ = precision_recall_curve(all_true, all_probs)
    pr_auc = auc(recall, precision)

    plt.figure(figsize=(8, 6))
    plt.plot(recall, precision, color='blue', lw=2, label=f'PR curve (AUC = {pr_auc:.3f})')
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('Precision-Recall Curve')
    plt.legend(loc="lower left")
    plt.show()

# 분석 실행
results = detailed_analysis(model, x[2], y[2])

"""모델 저장"""

torch.save({
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'train_history': train_history,
    'valid_history': valid_history,
    'best_model': best_model,
}, 'hotel_cancellation_model.pth')